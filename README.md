# STOCK-MARKET-INDICES-PREDICTION-USING-NEWS-HEADLINES
Stock prices are hard to predict based on some expertise through previous trends or past prices and hence need the help of artificial intelligence and data mining techniques. The volatility of stock prices depends on gains or losses of certain companies. News articles are one of the most important factors which influence the stock market. So for an efficient analysis of the current trends, new company’s product information, business growth etc., we propose to look at the daily news which represents factual information about the companies which could be ultimately used to predict the stock prices. Hence, we will be using news articles to predict the change in stock indices rather than predicting the prices by historical stock prices. We plan to perform sentiment analysis of the headlines and understand the investing insight through the emotion behind the headlines and predict whether the market feels good or bad about a stock, after which the output will be fed to various machine learning models to predict whether the price of stock indices will go up or not.
1. INTRODUCTION:
Stock prices are hard to predict based on some expertise through previous trends or past prices and
hence need the help of artificial intelligence and data mining techniques. The volatility of stock
prices depends on gains or losses of certain companies. News articles are one of the most important
factors which influence the stock market. So for an efficient analysis of the current trends, new
company’s product information, business growth etc., we propose to look at the daily news which
represents factual information about the companies which could be ultimately used to predict the
stock prices. Hence, we will be using news articles to predict the change in stock indices rather
than predicting the prices by historical stock prices. We plan to perform sentiment analysis of the
headlines and understand the investing insight through the emotion behind the headlines and
predict whether the market feels good or bad about a stock, after which the output will be fed to
various machine learning models to predict whether the price of stock indices will go up or not.

2. DATA DESCRIPTION:
The dataset consists of 4101 rows and 27 columns where the first column is assigned to Date and
second column is assigned to Label. This column consists of binary values where 1 represents if
the stock value increased or stayed the same and 0 represents if the stock value decreased. The
columns from 3 to 27 consists news headlines ranging from Top 1 to Top 25 corresponding with
the respective date mentioned. There are two types of data combined in the dataset:
• News data: the data consists of historical news headlines from Reddit World News
Channel. They are ranked by reddit users' votes, and only the top 25 headlines are
considered for a single date. All the news headlines are ranked from top to bottom based
on how hot they are.
(Range: 2008-06-08 to 2016-07-01)
• Stock data: Dow Jones Industrial Average (DJIA) is used to "prove the concept". This
data has been directly downloaded from the Yahoo Finance website.
(Range: 2008-08-08 to 2016-07-01)

3. MISSING DATA:
The dataset must be checked for missing data and obtain a total count. All the missing data must
then be replaced with blank/empty values. After parsing through the data set we find that column
25 has 1 cell with no data, column 26 has 3 cells with no data and column 27 has 3 cells with no
data. Hence, we replace the following missing values with a blank/empty value.

4. COMBINING THE NEWS HEADLINES:
To make the classification and prediction process easier we combine all the 25 news headlines in
a new column called “Combine” with respective to their dates. We also assign the values in the
Label column of 1 to UP variable and 0 to DOWN variable.

5. DATA PREPROCESSING:
It is an essential step in Natural Language Processing as depending on how well the data has been
preprocessed the results are seen. In this project the following pre-processing steps have been
considered based on the context and the necessity of the data:
• Removing Alpha Numeric Characters
• Lowercasing all the words
• Removing all the stop words

6. GENERATION OF WORD CLOUD:
Word clouds are used to easily produce a summary of large documents to visualize data. The
novelty visual representation of text data allows us to get high-level information about the current
analyzed document. Tags are usually single words, and the importance of each tag is shown with
font size or color. This format is useful for quickly perceiving the most prominent terms to
determine its relative prominence. Following is the word cloud generated by the analysis:
a. For Top UP [Positive] News Headlines:

b. For Top DOWN [Negative] News Headlines:

7. DATA SPLITTING:
Separating data into training and testing sets is an important part of evaluating data mining models.
Typically, when you separate a data set into a training set and testing set, most of the data is used
for training, and a smaller portion of the data is used for testing. Analysis Services randomly
samples the data to help ensure that the testing and training sets are similar. By using similar data
for training and testing, you can minimize the effects of data discrepancies and better understand
the characteristics of the model. After a model has been processed by using the training set, you
test the model by making predictions against the test set. Because the data in the testing set already
contains known values for the attribute that you want to predict, it is easy to determine whether
the model's guesses are correct. Here we used 80% of the data for training and 20% for testing.

8. ENCODING THE DATA:
Text data requires special preparation before we can start using it for predictive modeling. The
data must be parsed to remove words, called tokenization. Then the words need to be encoded as
integers or floating-point values to use then as an input to machine learning algorithms called
Vectorization. The scikit learn library offers easy-to-use tools to perform both tokenization and
vectorization of the text data. The Count Vectorizer provides a simple way to both tokenize a
collection of text document and build a vocabulary of known words but also to encode new
documents using that vocabulary. An encoded vector is returned with a length of the entire
vocabulary and an integer count for the number of times each word appeared in the document.

9. RANDOM FOREST CLASSIFIER:
Random Forest Classifier creates a set of decision trees from randomly selected subset of the
training set. It then aggregates the different votes from different decision trees to decide the final
class of the test object. It is an ensemble tree-based learning algorithm. Ensemble algorithms are
those which combines more than one algorithm of same or different kind for classifying
objects. The major advantage of this classifier is that it gives estimates of what variables that are
important in the classification. It generates an internal unbiased estimate of the generalization
error as the forest building progresses and can also handle thousands of input variables without
variable deletion.

10. LOGISTIC REGRESSION CLASSIFIER:
The coefficients of the logistic regression algorithm must be estimated from your training data.
This is done using maximum-likelihood estimation. Maximum-likelihood estimation is a common
learning algorithm used by a variety of machine learning algorithms, although it does make
assumptions about the distribution of your data. The best coefficients would result in a model that
would predict a value very close to 1 for the default class and a value very close to 0 for the other
class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks
values for the coefficients that minimize the error in the probabilities predicted by the model to
those in the data.

11. N-GRAM MODEL:
In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n
items from a given sample of text or speech. The items can be phonemes, syllables, letters, words,
or base pairs according to the application. The n-grams typically are collected from a text or speech
corpus. When the items are words, n-grams may also be called as Shingles. An n-gram model is a
probabilistic language model used for predicting the next time in a sequence of the form (n-1)
order Markov model. In this project we have run the Unigram, Bigram and Trigram Models using
Random Forest and Logistic Regression classifiers to compare the accuracy of our predictions.

12. RESULT ANALYSIS:
After the comparative analysis of different N-gram models we find the following results:
v. UNI-GRAM MODEL:
The Random Forest Classifier gives an accuracy of 52.86% while the Logistic
Regression Classifier gives an accuracy of 51.64%.
vi. BI-GRAM MODEL:
The Random Forest Classifier gives an accuracy of 50.66% while the Logistic
Regression Classifier also gives the same accuracy of 50.66%.
vii. TRI-GRAM MODEL:
The Random Forest Classifier gives an accuracy of 49.45% while the Logistic
Regression Classifier gives an accuracy of 48.96%.

13. CONCLUSION:
The accuracy of the model is the lowest for the trigram model and is the highest for unigram model.
Hence, for the prediction of stock indices from the news headlines, using the random forest
classifier of the unigram model, would give anyone the highest possibility of earning a profit if
invested on that particular company.
